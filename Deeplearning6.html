<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>전이학습</title>
    <link rel="stylesheet" href="style-Deeplearning6.css">
</head>
<body>
  <div class="content-container">
        <!-- 제목 -->
        <h1 class="left"> 전이학습 </h1>

        <!-- 전이학습 -->
        <div class="description-section">
            <h2> 전이학습 </h2>
            <div class="image-section.left">
              <img src="Deep learning6 images/1.png" alt="CNN 전이학습" class="deep-learning-image">
            </div>
        </div>
    <div class="description-section">
      <ul>
        <li>일반적으로 합성곱 신경망 기반의 딥러닝 모델을 제대로 훈련시키려면 많은 양의 데이터가 필요함.</li>
        <li>불행히도 충분한 큰 데이터셋을 얻는 것은 쉽지 않음.</li>
        <li>큰 데이터셋을 확보하려면 많은 돈과 시간이 필요하기 때문임.</li>
        <li>이러한 현실적인 어려움을 해결한 것이 전이 학습(transfer learning).</li>
        <li>전이 학습은 이미지넷(ImageNet)처럼 아주 큰 데이터셋에서 사전 훈련된 모델의 가중치를 가져와 우리가 해결하려는 과제에 맞게 보정해서 사용하는 것을 의미.</li>
        <li>이런 아주 큰 데이터셋을 사용하여 훈련된 모델을 사전 훈련된 모델(네트워크)라고 함.</li>
        <li>결과적으로 비교적 적은 수의 데이터를 가지고도 우리가 원하는 과제를 해결할 수 있음.</li>
      </ul>
    </div>    

      <div class="image-section.left">
          <img src="Deep learning6 images/2.png" alt="CNN 특성 추출 기법" class="deep-learning-image">
      </div>
      <div class="description-section">
          <h4>특성 추출 기법</h4>
          <ul>
              <li><strong>특성 추출(feature extractor):</strong> ImageNet 데이터셋으로 사전 훈련된 모델을 가져온 후 마지막에 완전연결층 부분만 새로 만듦.</li>
              <li>
                  학습할 때는 마지막 완전연결층(이미지의 카테고리를 결정하는 부분)만 학습하고 나머지 계층들은 학습되지 않도록 함.
              </li>
              <li>
                  특성 추출은 이미지 분류를 위해 두 부분으로 구성:
                  <ul>
                      <li><strong>합성곱층:</strong> 합성곱과 풀링층으로 구성.</li>
                      <li><strong>데이터 분류기(완전연결층):</strong> 추출된 특성을 입력받아 최종적으로 이미지에 대한 클래스를 분류하는 부분.</li>
                  </ul>
              </li>
              <li>
            사전 훈련된 네트워크의 합성곱층(가중치 고정)에 새로운 데이터로 증강시키고, 그 출력값 데이터를 분류기에 훈련시킴.
              </li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/3.png" alt="3" class="deep-learning-image">
      </div>
      <div class="description-section">
          <p>라이브러리와 GPU 할당</p>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/4.png" alt="4" class="deep-learning-image">
      </div>
      <div class="description-section">
          <ul>
              <li><strong>학습 데이터:</strong>
                  <ul>
                      <li><strong>데이터 증강:</strong> 크기 조정, 수평 뒤집기, 랜덤 영역 삭제.</li>
                      <li><strong>정규화:</strong> 데이터 값을 평균과 표준편차로 표준화.</li>
                      <li><strong>데이터 로더:</strong> 배치 크기=50, 데이터 순서를 섞음.</li>
                  </ul>
              </li>
              <li><strong>검증 데이터:</strong>
                  <ul>
                      <li><strong>데이터 증강 없음:</strong> 정규화만 수행.</li>
                      <li><strong>데이터 로더:</strong> 배치 크기=50, 데이터 순서를 유지.</li>
                  </ul>
              </li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/5.png" alt="5" class="deep-learning-image">
      </div>
      <div class="description-section">
          <ul>
              <li><code>weights='IMAGENET1K_V1'</code>:
                  <ul>
                      <li>PyTorch 최신 버전에서는 <code>pretrained=True</code> 대신 <code>weights</code> 매개변수를 사용.</li>
                      <li><code>'IMAGENET1K_V1'</code>: ImageNet 데이터셋으로 학습된 ResNet-18 모델의 가중치를 불러옴.</li>
                  </ul>
              </li>
          </ul>
      </div>      
      <div class="image-section.left">
          <img src="Deep learning6 images/6.png" alt="6" class="deep-learning-image">
      </div>
      <div class="description-section">
          <h4>1. 합성곱 층 (Conv2d):</h4>
          <pre>Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</pre>
          <ul>
              <li><strong>합성곱 연산을 수행하는 층.</strong></li>
              <li>입력 채널=128, 출력 채널=128, 커널 크기=3x3.</li>
              <li><code>stride=(1, 1)</code>: 필터가 1픽셀씩 이동.</li>
              <li><code>padding=(1, 1)</code>: 입력 이미지 가장자리에 패딩을 추가하여 출력 크기를 유지.</li>
              <li><code>bias=False</code>: 바이어스를 BatchNorm과 함께 사용하지 않음.</li>
          </ul>
          
          <h4>2. 배치 정규화 (BatchNorm2d):</h4>
          <pre>BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</pre>
          <ul>
              <li>채널별로 데이터를 정규화하여 학습 안정성 향상.</li>
              <li><code>eps</code>: 수치 안정성을 위해 추가되는 작은 값.</li>
              <li><code>momentum</code>: 이동 평균 계산 시 사용.</li>
              <li><code>affine=True</code>: 학습 가능한 스케일링과 이동 매개변수 사용.</li>
          </ul>
          
          <h4>3. 적용형 풀링층 (AdaptiveAvgPool2d):</h4>
          <pre>AdaptiveAvgPool2d(output_size=(1, 1))</pre>
          <ul>
              <li>입력 특징 맵의 크기를 1x1로 변환.</li>
              <li>입력 크기에 상관없이 고정된 출력 크기를 생성.</li>
          </ul>
          
          <h4>4. 완전 연결층 (Fully Connected):</h4>        
          <code>Linear(in_features=512, out_features=1000, bias=True)</code>
          <ul>
              <li>입력 특성 수=512, 출력 클래스 수=1000 (ImageNet 클래스 개수).</li>
              <li>모델의 최종 출력층으로, 각 클래스에 대한 점수 생성.</li>
          </ul>
      </div>   

      <div class="image-section.left">
          <img src="Deep learning6 images/7.png" alt="7" class="deep-learning-image">
      </div>
      <div class="description-section">
          <h4>1. 라이브러리 설치 및 임포트</h4>
          <pre>!pip install torchinfo | tail -n 1</pre>
          <ul>
              <li><strong>torchinfo:</strong> PyTorch 모델의 구조를 요약해서 출력하는 라이브러리.</li>
              <li><strong>tail -n 1:</strong> 설치 완료 메시지 한 줄만 출력.</li>
          </ul>
          <pre>from torchinfo import summary</pre>
          <ul>
              <li><code>torchinfo</code>의 <code>summary</code> 함수로 모델 구조를 시각화.</li>
          </ul>
          
          <h4>2. 모델 이동</h4>
          <pre>net = net.to(device)</pre>
          <ul>
              <li>모델(<code>net</code>)을 지정된 장치(<code>device</code>, GPU 또는 CPU)로 이동.</li>
          </ul>
          
          <h4>3. 모델 구조 요약</h4>
          <pre>summary(net, (128, 3, 112, 112))</pre>
          <ul>
              <li><code>net:</code> 모델 객체.</li>
              <li><code>(128, 3, 112, 112):</code> 입력 데이터 크기.
                  <ul>
                      <li>배치 크기=128, 채널 수=3 (RGB 이미지), 이미지 크기=112x112.</li>
                  </ul>
              </li>
              <li>모델 구조와 각 층의 출력 크기 및 매개변수 수를 요약해서 출력.</li>
          </ul>
          
          <h4>4. 출력 내용 해석</h4>
          <ul>
              <li><strong>Column 1: Layer</strong>
                  <ul>
                      <li>모델의 각 계층을 계층 깊이(<code>depth-idx</code>)와 함께 나열.</li>
                      <li>주요 계층:
                          <ul>
                              <li><code>Conv2d:</code> 합성곱 계층.</li>
                              <li><code>BatchNorm2d:</code> 배치 정규화 계층.</li>
                              <li><code>ReLU:</code> 활성화 함수 계층.</li>
                              <li><code>MaxPool2d:</code> 맥스 풀링 계층.</li>
                              <li><code>Sequential:</code> 여러 계층이 연속적으로 연결된 블록.</li>
                              <li><code>BasicBlock:</code> ResNet의 기본 구성 요소.</li>
                          </ul>
                      </li>
                  </ul>
              </li>
              <li><strong>Column 2: Output Shape</strong>
                  <ul>
                      <li>각 계층의 출력 텐서 크기:
                          <ul>
                              <li>예: <code>[128, 64, 56, 56]</code></li>
                              <li>배치 크기=128, 채널=64, 출력 이미지 크기=56x56.</li>
                          </ul>
                      
                      </li>
                      <li>계층마다 입력 데이터의 크기가 점진적으로 변환됨.</li>
                  </ul>
              </li>
              <li><strong>Column 3: Param #</strong>
                  <ul>
                      <li>각 계층의 학습 가능한 매개변수(Parameter) 개수.
                          <ul>
                              <li>예: <code>Conv2d 1-1:</code> 9,408개의 매개변수.</li>
                              <li><code>BatchNorm2d 1-2:</code> 128개의 매개변수.</li>
                          </ul>
                      </li>
                  </ul>
              </li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/8.png" alt="8" class="deep-learning-image">
      </div>
      <div class="description-section">
          <ul>
              <li><strong>기존 ResNet 모델:</strong> 최종 레이어를 CIFAR-10 데이터셋에 맞게 수정.</li>
              <li><strong>전이 학습 (Transfer Learning):</strong>
                  ResNet의 사전 학습된 가중치를 활용하여 새로운 데이터셋에 빠르게 적용.
              </li>
              <li><strong>최종 출력 클래스 수 (<code>n_output</code>):</strong>
                  간단히 바꾸는 방법으로 다양한 분류 문제에 ResNet을 적용할 수 있음.
              </li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/9.png" alt="9" class="deep-learning-image">
      </div>
      <div class="description-section">
          <ul>
              <li><strong>변경 전:</strong> ImageNet 데이터셋 (1,000개 클래스)을 위한 구조.</li>
              <li><strong>변경 후:</strong> CIFAR-10 데이터셋 (10개 클래스)을 위한 구조.</li>
              <li>ResNet 모델의 최종 레이어 수정만으로 새로운 데이터셋에 맞춰 학습 가능.</li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/10.png" alt="10" class="deep-learning-image">
      </div>
      <div class="description-section">
          <h4>주요 변경 사항</h4>
          <ul>
              <li><strong>AdaptiveAvgPool2d:</strong>
                  <ul>
                      <li>이전 계층의 출력 특징을 <code>1×1</code>로 축소.</li>
                      <li>입력 데이터 크기에 관계없이 일정한 출력 크기를 제공.</li>
                  </ul>
              </li>
              <li><strong>Linear (Fully Connected):</strong>
                  <ul>
                      <li><code>out_features</code>가 <code>1,000</code>에서 <code>10</code>으로 변경:
                          <ul>
                              <li>ResNet의 기본 ImageNet 분류용 FC 레이어를 CIFAR-10에 맞게 수정.</li>
                          </ul>
                      </li>
                      <li><strong>매개변수 수가 줄어듦:</strong>
                          <ul>
                              <li>기존: <code>512 × 1,000 + 1,000 = 513,000</code></li>
                              <li>변경 후: <code>512 × 10 + 10 = 5,130</code></li>
                          </ul>
                      </li>
                  </ul>
              </li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/11.png" alt="11" class="deep-learning-image">
      </div>
      <div class="description-section">
          <pre>
              torch.manual_seed(123)
              torch.cuda.manual_seed(123)
          </pre>
          <ul>
              <li>랜덤 시드를 고정하여 재현 가능성 확보.</li>
              <li>CPU와 GPU에서 동일한 초기화 보장.</li>
          </ul>
          <pre>
              model = models.resnet18(weights='IMAGENET1K_V1')
              fc_in_features = model.fc.in_features
              model.fc = nn.Linear(fc_in_features, n_output)
          </pre>
          <ul>
              <li><strong>ResNet-18 모델 로드:</strong>
                  <ul>
                      <li><code>weights='IMAGENET1K_V1'</code>: ImageNet으로 학습된 가중치를 로드.</li>
                  </ul>
              </li>
              <li><strong>최종 Fully Connected 레이어 수정:</strong>
                  <ul>
                      <li><code>model.fc</code>: 기존의 1,000개 출력 클래스 → <code>n_output</code> 클래스(CIFAR-10 클래스)로 변경.</li>
                      <li>입력 특징 수(<code>fc_in_features</code>)는 그대로 사용.</li>
                  </ul>
              </li>
          </ul>
          <pre>
              lr = 0.001
              criterion = nn.CrossEntropyLoss()
              optimizer = torch.optim.Adam(model.parameters(), lr=lr)
          </pre>
          <ul>
              <li><strong>Learning Rate (lr):</strong>
                  <ul>
                      <li>학습률 설정 (<code>lr = 0.001</code>).</li>
                  </ul>
              </li>
              <li><strong>Loss Function:</strong>
                  <ul>
                      <li><code>CrossEntropyLoss</code>: 다중 클래스 분류를 위한 손실 함수.</li>
                  </ul>
              </li>
              <li><strong>Optimizer:</strong>
                  <ul>
                      <li><code>Adam</code>: 학습 가속과 적응적 학습률 조정에 효과적.</li>
                      <li><code>model.parameters()</code>: 모델의 모든 가중치와 바이어스를 업데이트.</li>
                  </ul>
              </li>
          </ul>
          <pre>
              history = np.zeros((0, 5))
          </pre>
          <ul>
              <li>학습 결과를 저장할 배열 생성.</li>
              <li>각 항목: [에포크, train_loss, train_accuracy, test_loss, test_accuracy].</li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/12.png" alt="12" class="deep-learning-image">
      </div>
      <div class="description-section">
          <ul>
              <li>이전 코드와 동일.(설명 생략)</li>
          </ul>
      </div>

      <div class="image-section.left">
          <img src="Deep learning6 images/13.png" alt="13" class="deep-learning-image">
      </div>
      <div class="description-section">
          <h4>1. 손실 곡선 (Loss Curve)</h4>
          <h5>그래프 해석</h5>
          <ul>
              <li><strong>훈련 손실 (Train error):</strong>
                  <ul>
                      <li>초기에는 손실이 높음.</li>
                      <li>에포크가 진행됨에 따라 꾸준히 감소.</li>
                      <li>최종적으로 매우 낮은 수준의 손실을 기록.</li>
                  </ul>
              </li>
              <li><strong>테스트 손실 (Test error):</strong>
                  <ul>
                      <li>초기에는 빠르게 감소.</li>
                      <li>감소 이후부터는 감소가 더디며 약간의 변동.</li>
                      <li>일정 수준에서 안정화.</li>
                  </ul>
              </li>
              <li><strong>훈련 vs 테스트 손실:</strong>
                  <ul>
                      <li>훈련 손실이 테스트 손실보다 낮음.</li>
                      <li>약간의 차이는 있지만, 과적합 정도는 심각하지 않음.</li>
                  </ul>
              </li>
          </ul>
          
          <h4>2. 정확도 곡선 (Accuracy Curve)</h4>
          <h5>그래프 해석</h5>
          <ul>
              <li><strong>훈련 정확도 (Train Accuracy):</strong>
                  <ul>
                      <li>초기에는 낮은 정확도로 시작.</li>
                      <li>에포크가 진행됨에 따라 꾸준히 상승.</li>
                      <li>최종적으로 약 97%에 도달.</li>
                  </ul>
              </li>
              <li><strong>테스트 정확도 (Test Accuracy):</strong>
                  <ul>
                      <li>초기에는 훈련 정확도와 유사한 수준.</li>
                      <li>이후에는 상승이 일관해지며, 약 88~90% 수준에서 안정화.</li>
                  </ul>
              </li>
              <li><strong>훈련 vs 테스트 정확도:</strong>
                  <ul>
                      <li>훈련 정확도가 테스트 정확도보다 높음.</li>
                      <li>테스트 정확도의 변동이 훈련 정확도보다 큼.</li>
                  </ul>
              </li>
          </ul>
      </div>     
</body>
</html>
