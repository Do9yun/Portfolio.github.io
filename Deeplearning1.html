<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>딥러닝의 기초 이론</title>
    <link rel="stylesheet" href="style-Deeplearning1.css">
</head>
<body>
    <div class="content-container">
        <!-- 제목 -->
        <h1>딥러닝의 기초 이론</h1>

        <!-- 설명 섹션 -->
        <div class="description-section">
            <h2>퍼셉트론</h2>
            <p>
                퍼셉트론 : 다수의 신호(흐름이 있는)를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 입력으로 받아 ‘흐른다/안 흐른다(1 또는 0)’는 정보를 앞으로 전달하는 원리로 작동.<br>
                오늘날 인공 신경망에서 이용하는 구조(입력층, 출력층, 가중치로 구성된 구조)는 프랭크 로젠블라트(Frank Rosenblatt)가 1957년에 고안한 퍼셉트론이라는 선형 분류기.<br>
                이 퍼셉트론은 오늘날 신경망(딥러닝)의 기원이 되는 알고리즘.
            </p>
        </div>

        <!-- 퍼셉트론 이미지 -->
        <div class="image-section">
            <img src="Deep learning images/1.png" alt="퍼셉트론" class="deep-learning-image">
        </div>

        <!-- 퍼셉트론의 활용 -->
        <div class="description-section">
            <h2>퍼셉트론의 활용</h2>
            <p>
                <strong>1. AND 게이트</strong><br>
                AND 게이트는 모든 입력이 ‘1’일 때 작동.<br>
                즉, 입력 중 어떤 하나라도 ‘0’을 갖는다면 작동을 멈추는데, 이를 진리표로 표현하면 다음 표와 같음.
            </p>
        </div>

        <!-- AND 게이트 이미지 -->
        <div class="image-section">
            <img src="Deep learning images/2.png" alt="AND 게이트 진리표" class="deep-learning-image">
        </div>

        <!-- OR 게이트 -->
        <div class="description-section">
            <p>
                <strong>2. OR 게이트</strong><br>
                OR 게이트는 입력에서 둘 중 하나만 ‘1’이거나 둘 다 ‘1’일 때 작동.<br>
                즉, 입력 모두가 ‘0’을 갖는 경우를 제외한 나머지가 모두 ‘1’ 값을 갖는데, 이를 진리표로 표현하면 다음 표와 같음.
            </p>
        </div>

        <!-- OR 게이트 이미지 -->
        <div class="image-section">
            <img src="Deep learning images/3.png" alt="OR 게이트 진리표" class="deep-learning-image">
        </div>

        <!-- XOR 게이트 -->
        <div class="description-section">
            <p>
                <strong>3. XOR 게이트</strong><br>
                데이터가 비선형적으로 분리되기 때문에 제대로 된 분류가 어려움.<br>
                즉, 단층 퍼셉트론에서는 AND, OR 연산에 대해서는 학습이 가능하지만, XOR에 대해서는 학습이 불가능.
            </p>
        </div>

        <!-- XOR 게이트 이미지 -->
        <div class="image-section">
            <img src="Deep learning images/4.png" alt="XOR 게이트 진리표" class="deep-learning-image">
        </div>

        <!-- 다층 퍼셉트론 -->
        <div class="description-section">
            <p>
                이를 극복하는 방안으로 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 두어
                비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론(Multilayer Perceptron)을 고안했음.<br>
                이때 입력층과 출력층 사이에 은닉층이 여러 개 있는 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 하며, 심층 신경망을 다른 이름으로 딥러닝이라고 함.
            </p>
        </div>

        <!-- 다층 퍼셉트론 이미지 -->
        <div class="image-section">
            <img src="Deep learning images/main.webp" alt="다층 퍼셉트론" class="deep-learning-image">
        </div>

        <!-- 딥러닝의 구성 -->
        <div class="description-section">
            <h2>딥러닝의 구성</h2>
            <p>
                딥러닝은 <strong>인공신경망(Artificial Neural Network)</strong>을 기반으로 한 기법으로, 데이터를 처리하고 학습하기 위해 여러 층(Layers)으로 구성된 구조를 사용합니다.
            </p>
            <ul>
                <li>
                    <strong>입력층 (Input Layer):</strong> 모델에 데이터를 입력하는 층.<br>
                    각 뉴런은 입력 데이터의 각 특징(feature)을 나타냄.
                </li>
                <li>
                    <strong>은닉층 (Hidden Layers):</strong> 입력 데이터를 변환하고 패턴을 학습하는 주요 계층.<br>
                    <strong>활성화 함수(Activation Function)</strong>를 통해 비선형 변환을 수행.<br>
                    층의 개수와 뉴런 수에 따라 모델의 복잡도와 학습 능력이 결정됨.
                </li>
                <li>
                    <strong>출력층 (Output Layer):</strong> 모델의 최종 결과를 출력하는 층.<br>
                    회귀 문제에서는 실수 값을, 분류 문제에서는 확률 값을 반환.
                </li>
            </ul>
        </div>

        <!-- 활성화 함수 -->
        <div class="description-section">
            <h2>활성화 함수</h2>
            <p>
                먼저 활성화 함수는 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수입니다.<br>
                활성화 함수로는 시그모이드(sigmoid), 하이퍼볼릭 탄젠트(hyperbolic tangent), 렐루(ReLU) 함수 등이 있습니다.
            </p>
        </div>

        <!-- 시그모이드 함수 -->
        <div class="description-section">
            <h3>시그모이드 함수</h3>
            <p>
                시그모이드 함수는 선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형합니다.<br>
                주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용되며, 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는
                ‘기울기 소멸 문제(vanishing gradient problem)’가 발생하여 딥러닝 모델에서는 잘 사용하지 않습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/6.png" alt="시그모이드 함수" class="deep-learning-image">
            </div>
        </div>

        <!-- 하이퍼볼릭 탄젠트 함수 -->
        <div class="description-section">
            <h3>하이퍼볼릭 탄젠트 함수</h3>
            <p>
                하이퍼볼릭 탄젠트 함수는 선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형합니다.<br>
                시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만, 기울기 소멸 문제는 여전히 발생합니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/7.png" alt="하이퍼볼릭 탄젠트 함수" class="deep-learning-image">
            </div>
        </div>

        <!-- 렐루 함수 -->
        <div class="description-section">
            <h3>렐루 함수</h3>
            <p>
                경사 하강법(gradient descent)에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점이 있습니다.<br>
                렐루 함수는 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠릅니다.<br>
                음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소할 수 있습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/8.png" alt="렐루 함수" class="deep-learning-image">
            </div>
        </div>

        <!-- 리키 렐루 함수 -->
        <div class="description-section">
            <h3>리키 렐루 함수</h3>
            <p>
                리키 렐루(Leaky ReLU) 함수는 입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환합니다.<br>
                이렇게 하면 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/9.png" alt="리키 렐루 함수" class="deep-learning-image">
            </div>
        </div>

        <!-- 손실 함수 -->
        <div class="description-section">
            <h2>손실 함수</h2>
            <p>
                손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표라고 할 수 있습니다.<br>
                이 값이 클수록 많이 틀렸다는 의미이고, 이 값이 ‘0’에 가까우면 완벽하게 추정할 수 있다는 의미입니다.<br>
                대표적인 손실 함수로는 평균 제곱 오차(Mean Squared Error, MSE)와 크로스 엔트로피 오차(Cross Entropy Error, CEE)가 있습니다.
            </p>
        </div>

        <!-- 평균 제곱 오차 -->
        <div class="description-section">
            <h3>평균 제곱 오차</h3>
            <p>
                실제 값과 예측 값의 차이(error)를 제곱하여 평균을 낸 것이 평균 제곱 오차(MSE)입니다.<br>
                이 값이 작을수록 예측력이 좋다는 것을 의미하며, 회귀에서 손실 함수로 주로 사용됩니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/10.png" alt="평균 제곱 오차" class="deep-learning-image">
            </div>
        </div>

        <!-- 크로스 엔트로피 오차 -->
        <div class="description-section">
            <h3>크로스 엔트로피 오차</h3>
            <p>
                크로스 엔트로피는 분류 모델이 얼마나 잘 수행되는지 측정하기 위해 사용됩니다.<br>
                0과 1 사이로 측정하며, 주로 결과값이 3개 이상일 때 사용됩니다.<br>
                결과값이 2개인 경우 Binary Cross Entropy Loss를 사용할 수 있습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/11.png" alt="크로스 엔트로피 오차" class="deep-learning-image">
            </div>
        </div>

        <!-- 딥러닝 학습 -->
        <div class="description-section">
            <h2>딥러닝 학습</h2>
            <div class="image-section">
                <img src="Deep learning images/12.png" alt="딥러닝 학습 과정" class="deep-learning-image">
            </div>
            <p>
                <strong>첫 번째 단계</strong><br>
                순전파(feedforward)는 네트워크에 훈련 데이터가 들어올 때 발생하며, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 지나갑니다.<br>
                모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환(가중합 및 활성화 함수)을 적용하여 다음 층(은닉층)의 뉴런으로 전송하는 방식입니다.<br>
                데이터를 통과시키며 최종 층(출력층)에 도달하게 됩니다.
            </p>
            <p>
                <strong>두 번째 단계</strong><br>
                손실 함수로 네트워크의 예측 값과 실제 값의 차이(손실, 오차)를 추정합니다.<br>
                손실이 계산되면 역으로 전파(출력층 → 은닉층 → 입력층)되며, 이를 역전파(backpropagation)라고 합니다.<br>
                예측 값과 실제 값 차이를 각 뉴런의 가중치로 미분한 후 기존 가중치 값에서 빼며 학습이 진행됩니다.
            </p>
        </div>

        <div class="description-section">
            <h2>은닉층이 분류에 미치는 영향</h2>
            <p>
                딥러닝의 핵심은 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것입니다.
                다음 그림과 같이 활성화 함수가 적용된 은닉층 개수가 많을수록 데이터 분류가 잘되고 있음을 볼 수 있습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/13.png" alt="은닉층이 분류에 미치는 영향" class="deep-learning-image">
            </div>
            <h3>은닉층이 많을수록 과적합 문제 발생</h3>
            <p>
                과적합(over-fitting)은 훈련 데이터를 과하게 학습해서 발생합니다.
                훈련 데이터를 과하게 학습했기 때문에 예측 값과 실제 값 차이인 오차가 감소하지만, 검증 데이터에 대해서는 오차가 증가할 수 있습니다.
                이러한 관점에서 과적합은 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상을 의미합니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/14.png" alt="과적합 문제" class="deep-learning-image">
            </div>
            <h3>과적합을 해결하는 방법 중 하나</h3>
            <p>
                드롭아웃(dropout) 신경망 모델이 과적합되는 것을 피하기 위한 방법으로, 학습 과정 중 임의로 일부 노드들을 학습에서 제외시킵니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/15.png" alt="드롭아웃" class="deep-learning-image">
            </div>
            <h3>기울기 소실 문제</h3>
            <p>
                기울기 소실 문제는 은닉층이 많은 신경망에서 주로 발생하는데, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상입니다.
                즉, 기울기가 소멸되기 때문에 학습되는 양이 ‘0’에 가까워져 학습이 더디게 진행되다 오차를 더 줄이지 못하고 그 상태로 수렴하는 현상입니다.
                기울기 소멸 문제는 렐루 활성화 함수를 사용하면 해결할 수 있습니다.
            </p>
            <div class="image-section">
                <img src="Deep learning images/16.png" alt="기울기 소실 문제" class="deep-learning-image">
            </div>
        </div>
    </div>
</body>
</html>
